{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMkcOWCMzjzuNdqM4hJiIjf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Dog-Cat Classification\n","21/09/2022 by LimBus"],"metadata":{"id":"HCxqwkiDLvSu"}},{"cell_type":"markdown","source":["**Pipeline**\n","*   Get & Prepare dataset\n","*   Build the model\n","*   Train the model\n","*   Evaluate the trained model"],"metadata":{"id":"g3qfWd3WMqOH"}},{"cell_type":"markdown","source":["## Get & Prepare dataset"],"metadata":{"id":"n-PgkIcaOQrg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SMJ9TbXbLh1N"},"outputs":[],"source":["# import libraries\n","\n","import os\n","import zipfile\n","import matplotlib.image as mpimg\n","\n","print('cell complete!')"]},{"cell_type":"code","source":["# load dataset\n","\n","!wget --no-check-certificate \\\n","  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n","  -O /tmp/cats_and_dogs_filtered.zip\n","\n","print('cell complete!')"],"metadata":{"id":"F8KqmimlM2U-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# unzip\n","\n","local_zip = '/tmp/cats_and_dogs_filtered.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('/tmp')\n","zip_ref.close()\n","\n","print('cell complete!')"],"metadata":{"id":"Kcr0X6H3M9v4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set directory path\n","\n","base_dir = '/tmp/cats_and_dogs_filtered'\n","train_dir = os.path.join(base_dir, 'train')\n","validation_dir = os.path.join(base_dir, 'validation')\n","\n","# Directory with our training cat pictures\n","train_cats_dir = os.path.join(train_dir, 'cats')\n","\n","# Directory with our training dog pictures\n","train_dogs_dir = os.path.join(train_dir, 'dogs')\n","\n","# Directory with our validation cat pictures\n","validation_cats_dir = os.path.join(validation_dir, 'cats')\n","\n","# Directory with our validation dog pictures\n","validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n","\n","print('cell complete!')"],"metadata":{"id":"ck1rsv_9NAMV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get list of file names\n","\n","train_cat_fnames = os.listdir(train_cats_dir)\n","print(train_cat_fnames[:10])\n","\n","train_dog_fnames = os.listdir(train_dogs_dir)\n","train_dog_fnames.sort()\n","print(train_dog_fnames[:10])\n","\n","print('cell complete!')"],"metadata":{"id":"W9mXcwPuNDmq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print total lenght of train/validation set\n","\n","print('total training cat images: {}'.format(len(train_cat_fnames)))\n","print('total training dog images: {}'.format(len(train_dog_fnames)))\n","print('total validation cat images: {}'.format(len(os.listdir(validation_cats_dir))))\n","print('total validation dog images: {}'.format(len(os.listdir(validation_dogs_dir))))\n","\n","print('cell complete!')"],"metadata":{"id":"VXM7MUT0NIoV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data visualization\n","import matplotlib.pyplot as plt\n","\n","nrows = 4\n","ncols = 4\n","\n","pic_index = 0\n","\n","fig = plt.gcf()\n","fig.set_size_inches(ncols * 4, nrows * 4)\n","\n","pic_index += 8\n","next_cat_pix = [os.path.join(train_cats_dir, fname)\n","                for fname in train_cat_fnames[pic_index-8:pic_index]]\n","next_dog_pix = [os.path.join(train_dogs_dir, fname)\n","                for fname in train_dog_fnames[pic_index-8:pic_index]]\n","\n","for i, img_path in enumerate(next_cat_pix+next_dog_pix):\n","  sp = plt.subplot(nrows, ncols, i+1)\n","  sp.axis('Off')\n","\n","  img = mpimg.imread(img_path)\n","  plt.imshow(img)\n","\n","plt.show()\n","\n","print('cell complete!')"],"metadata":{"id":"jg2lSlXkNLMW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torchvision\n","from torchvision import datasets, transforms\n","\n","#transformations\n","train_transforms = transforms.Compose([transforms.Resize((225,225)),\n","                                       transforms.ToTensor(),                               \n","                                       torchvision.transforms.Normalize(\n","                                           mean=[0.485, 0.456, 0.406],\n","                                           std=[0.229, 0.224, 0.225],),\n","                                       ])\n","test_transforms = transforms.Compose([transforms.Resize((225,225)),\n","                                      transforms.ToTensor(),\n","                                      torchvision.transforms.Normalize(\n","                                          mean=[0.485, 0.456, 0.406],\n","                                          std=[0.229, 0.224, 0.225],),\n","                                      ])\n","\n","#datasets\n","train_data = datasets.ImageFolder(train_dir,transform=train_transforms)\n","test_data = datasets.ImageFolder(validation_dir,transform=test_transforms)\n","\n","#dataloader\n","trainloader = torch.utils.data.DataLoader(train_data, shuffle = True, batch_size=20)\n","testloader = torch.utils.data.DataLoader(test_data, shuffle = True, batch_size=20)"],"metadata":{"id":"jx__DeLJNQq2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from torch.autograd import Variable\n","\n","examples = enumerate(trainloader)\n","print(\"len: \", len(trainloader))\n","idx, batch = next(examples)\n","inp_0 = Variable(batch[0][0]).squeeze()\n","print(\"sample shape: \", inp_0.size())\n","f, axr = plt.subplots(1,1)\n","axr.imshow(transforms.ToPILImage()(inp_0))\n","print('cell complete!')"],"metadata":{"id":"5ZzThxXoNhTm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Build the model"],"metadata":{"id":"MuzckvE5OhsD"}},{"cell_type":"code","source":["torch.cuda.is_available()"],"metadata":{"id":"2A1lSQDjNn81"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision import datasets, models, transforms\n","import torch.nn as nn\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = models.vgg16(pretrained=True)\n","\n","#freeze all params\n","for params in model.parameters():\n","  params.requires_grad = False\n","\n","#add a new final layer\n","nr_filters = model.classifier[0].in_features  #number of input features of last layer\n","model.classifier = nn.Sequential(\n","    nn.Linear(nr_filters, 1, bias=False))\n","model.classifier[0].weight.data.zero_()\n","model = model.to(device)\n","model"],"metadata":{"id":"eMU32ZMIOnev"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["examples = enumerate(trainloader)\n","print(\"len: \", len(trainloader))\n","idx, batch = next(examples)\n","inp_0 = Variable(batch[0][0]).squeeze()\n","print(\"sample shape: \", inp_0.size())"],"metadata":{"id":"-ir0bir4Qvyt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchsummary import summary\n","summary(model, inp_0.shape)"],"metadata":{"id":"WxIUzhIEOuZ_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train the model"],"metadata":{"id":"9hh5xcXFRaWd"}},{"cell_type":"code","source":["from torch.nn.modules.loss import BCEWithLogitsLoss\n","\n","# loss\n","loss_fn = BCEWithLogitsLoss() # binary cross entropy with sigmoid, so no need to use sigmoid in the model\n","\n","# optimizer\n","optimizer = torch.optim.SGD(model.classifier.parameters(), lr=.001) \n","optimizer"],"metadata":{"id":"LFJIAdiBQz2e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hyperparameters setting\n","losses = []\n","val_losses = []\n","\n","epoch_train_losses = []\n","epoch_test_losses = []\n","\n","n_epochs = 10\n","early_stopping_tolerance = 3\n","early_stopping_threshold = 0.03"],"metadata":{"id":"C7IUY_G4Spjd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","for epoch in range(n_epochs):\n","  epoch_loss = 0\n","  for i ,data in tqdm(enumerate(trainloader), total = len(trainloader)): # iterate ove batches\n","    x_batch , y_batch = data\n","    x_batch = x_batch.to(device) # move to gpu\n","    y_batch = y_batch.unsqueeze(1).float() # convert target to same nn output shape\n","    y_batch = y_batch.to(device) # move to gpu\n","\n","    # enter train mode\n","    model.train()\n","\n","    # make prediction\n","    yhat = model(x_batch)\n","\n","    # compute loss\n","    loss = loss_fn(yhat,y_batch)\n","\n","    # conpute grad\n","    optimizer.zero_grad()\n","    loss.backward()\n","\n","    # update parameters\n","    optimizer.step()\n","\n","    epoch_loss += loss/len(trainloader)\n","    losses.append(loss)\n","    \n","  epoch_train_losses.append(epoch_loss)\n","  print('Epoch : {}, train loss : {}'.format(epoch+1,epoch_loss))\n","\n","  # validation doesnt requires gradient\n","  with torch.no_grad():\n","    cum_loss = 0\n","    for x_batch, y_batch in testloader:\n","      x_batch = x_batch.to(device)\n","      y_batch = y_batch.unsqueeze(1).float() # convert target to same nn output shape\n","      y_batch = y_batch.to(device)\n","\n","      # model to eval mode\n","      model.eval()\n","\n","      yhat = model(x_batch)\n","      val_loss = loss_fn(yhat,y_batch)\n","      cum_loss += val_loss/len(testloader)\n","      val_losses.append(val_loss.item())\n","\n","\n","    epoch_test_losses.append(cum_loss)\n","    print('Epoch : {}, val loss : {}'.format(epoch+1,cum_loss))  \n","    \n","    best_loss = min(epoch_test_losses)\n","    print('Epoch : {}, best loss : {}'.format(epoch+1,best_loss))\n","    \n","    # save best model\n","    if cum_loss <= best_loss:\n","      best_model_wts = model.state_dict()\n","    \n","    # early stopping\n","    early_stopping_counter = 0\n","    if cum_loss > best_loss:\n","      early_stopping_counter +=1\n","\n","    if (early_stopping_counter == early_stopping_tolerance) or (best_loss <= early_stopping_threshold):\n","      print(\"Terminating: early stopping\")\n","      break # terminate training"],"metadata":{"id":"749Uv3ouSY3z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#load best model\n","model.load_state_dict(best_model_wts)\n","checkpoint_dict = {\n","    'model_state_dict' : model.state_dict(),\n","    'optimizer_state_dict' : optimizer.state_dict()}"],"metadata":{"id":"KJ4IA7oaS-06"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(checkpoint_dict, '/content/mymodel.pth')"],"metadata":{"id":"H59dzvH1S_ly"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluate the trained model"],"metadata":{"id":"DGNvBzmMRdZL"}},{"cell_type":"code","source":["class UnNormalize(object):\n","    def __init__(self, mean, std):\n","        self.mean = mean\n","        self.std = std\n","\n","    def __call__(self, tensor):\n","        \"\"\"\n","        Args:\n","            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n","        Returns:\n","            Tensor: Normalized image.\n","        \"\"\"\n","        for t, m, s in zip(tensor, self.mean, self.std):\n","            t.mul_(s).add_(m)\n","            # The normalize code -> t.sub_(m).div_(s)\n","        return tensor\n","  \n","unorm = UnNormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"],"metadata":{"id":"1s1BrlMaRhZG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from torchvision import transforms\n","\n","def inference(test_data):\n","  idx = torch.randint(1, len(test_data), (1,))\n","  sample = torch.unsqueeze(test_data[idx][0], dim=0).to(device)\n","\n","  p = torch.sigmoid(model(sample))\n","  p = p.to('cpu').data.item()\n","  if p < 0.5:\n","    print(f\"Prediction : Cat {1-p} & Dog {p}\")\n","  else:\n","    print(f\"Prediction : Dog {p} & Cat {1-p}\")\n","\n","  plt.imshow(transforms.ToPILImage()(unorm(test_data[idx][0])))\n","\n","inference(test_data)"],"metadata":{"id":"KKRuNMXkUzO-"},"execution_count":null,"outputs":[]}]}